{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Tiền xử lý dữ liệu đầu vào**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from torchtext import transforms as T\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Chuẩn hóa dữ liệu văn bản đầu vào**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phần xác định các cụm viết tắt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTRACTIONS = {\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"can not\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"she's\":\"she is\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"i'd\": \"I would\",\n",
    "    \"i'll\": \"I will\",\n",
    "    \"i'm\": \"I am\",\n",
    "    \"i've\": \"I have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"there're\": \"there are\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'll\":\"we will\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"who'd\": \"who would\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who're\": \"who are\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"wasn't\":\"was not\",\n",
    "    \"it'll\":\"it will\",\n",
    "    \"tom's\":\"tom is\", \n",
    "    \"one's\":\"one is\",\n",
    "    \"somebody's\": \"sombody is\",\n",
    "    \"someone's\": \"someone is\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"why're\": \"why are\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"here's\": 'here is',\n",
    "    \"could've\": \"could have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"that'll\": \"that will\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chuẩn hóa các dấu câu, viết tắt, viết hoa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Thực hiện kèm Tokenizer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"E:\\Đồ án 2\\vie-eng\\vie.txt\"\n",
    "\n",
    "def expand_match(contraction, contractions_dict= CONTRACTIONS):\n",
    "    match = contraction.group(0)\n",
    "    first_char = match[0]\n",
    "    expanded = contractions_dict.get(match.lower())\n",
    "    if expanded:\n",
    "        if first_char.isupper():\n",
    "            expanded = expanded[0].upper() + expanded[1:]\n",
    "        return expanded\n",
    "    return match\n",
    "\n",
    "def replace_time(match):\n",
    "        hours = match.group('hours')\n",
    "        minutes = match.group('minutes')\n",
    "        \n",
    "        components = []\n",
    "        \n",
    "        if hours:\n",
    "            hours = int(hours)\n",
    "            hour_word = \"hour\" if hours == 1 else \"hours\"\n",
    "            components.append(f\"{hours} {hour_word}\")\n",
    "        \n",
    "        if minutes:\n",
    "            minutes = int(minutes)\n",
    "            # Chuẩn hóa số phút\n",
    "            if minutes < 10 and ':' in match.group():\n",
    "                minutes = minutes * 10\n",
    "            minute_word = \"minute\" if minutes == 1 else \"minutes\"\n",
    "            components.append(f\"{minutes} {minute_word}\")\n",
    "        \n",
    "        return \" \".join(components)\n",
    "\n",
    "def text_normalize(file_path, contractions_dict= CONTRACTIONS):\n",
    "    src_data = pd.read_csv(file_path, delimiter=\"\\t\", header= None)\n",
    "    src_data.columns = ['src', 'tgt', 'ref']\n",
    "    src_data = src_data.drop(columns=['ref'])\n",
    "\n",
    "    normal_data = {}\n",
    "    normal_data['src'] = []\n",
    "    for row in src_data['src']:\n",
    "    #Xóa dấu . cuối câu\n",
    "        sentence = re.sub(r'\\.$', '', row.strip())\n",
    "        #Xóa khoảng trắng thừa\n",
    "        sentence = re.sub(r'\\s*,\\s*', ', ', sentence)\n",
    "        #Xóa dấu phẩy thừa\n",
    "        sentence = re.sub(r',+', ',', sentence)\n",
    "        #Tách dấu câu còn lại để lấy ngữ cảnh\n",
    "        sentence = re.sub(r'([.!?])', r' \\1', sentence)\n",
    "\n",
    "        # Thực hiện thay thế viết tắt trong text\n",
    "        pattern = re.compile(r'\\b(' + '|'.join(contractions_dict.keys()) + r')\\b', re.IGNORECASE)    \n",
    "        sentence = pattern.sub(expand_match, sentence)\n",
    "        \n",
    "        patterns = [\n",
    "        # h:m hoặc h:mm\n",
    "        r'(?P<hours>\\d{1,2}):(?P<minutes>\\d{1,2})',\n",
    "        # Các định dạng khác\n",
    "        r'(?P<hours>\\d+)\\s*h(?:\\s*(?P<minutes>\\d+)\\s*m)?'\n",
    "        ]\n",
    "        for part in patterns:\n",
    "            sentence = re.sub(part, replace_time, sentence, flags=re.IGNORECASE)\n",
    "        #Xóa dấu \" thừa ra\n",
    "        sentence = re.sub(r\"'\", '', sentence)\n",
    "        sentence = sentence.strip(r'\"')\n",
    "        sentence = sentence.strip(r\"'\")\n",
    "\n",
    "        #sentence = tokenizer(sentence)\n",
    "        normal_data['src'].append(sentence)\n",
    "    \n",
    "    normal_data['tgt'] = []\n",
    "    for row in src_data['tgt']:\n",
    "    #Xóa dấu . cuối câu\n",
    "        sentence = re.sub(r'\\.$', '', row.strip())\n",
    "        #Xóa khoảng trắng thừa\n",
    "        sentence = re.sub(r'\\s*,\\s*', ', ', sentence)\n",
    "        #Xóa dấu phẩy thừa\n",
    "        sentence = re.sub(r',+', ',', sentence)\n",
    "        #Tách dấu câu còn lại để lấy ngữ cảnh\n",
    "        sentence = re.sub(r'([.!?])', r' \\1', sentence)\n",
    "        # Thực hiện thay thế trong text\n",
    "        pattern = re.compile(r'\\b(' + '|'.join(contractions_dict.keys()) + r')\\b', re.IGNORECASE)    \n",
    "        sentence = pattern.sub(expand_match, sentence)\n",
    "        \n",
    "        patterns = [\n",
    "        # h:m hoặc h:mm\n",
    "        r'(?P<hours>\\d{1,2}):(?P<minutes>\\d{1,2})',\n",
    "        \n",
    "        # Các định dạng khác\n",
    "        r'(?P<hours>\\d+)\\s*h(?:\\s*(?P<minutes>\\d+)\\s*m)?',\n",
    "        ]\n",
    "        for part in patterns:\n",
    "            sentence = re.sub(part, replace_time, sentence, flags=re.IGNORECASE)\n",
    "        #Xóa dấu \" thừa ra\n",
    "        sentence = re.sub(r\"'\", '', sentence)\n",
    "        sentence = sentence.strip(r'\"')\n",
    "        sentence = sentence.strip(r\"'\")\n",
    "        #sentence = tokenizer(sentence)\n",
    "        normal_data['tgt'].append(sentence)\n",
    "\n",
    "    return normal_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Quá trình vectorize dữ liệu text**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xây dựng từ điển (vocab) cho bản gốc và bản dịch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_src_tokens(dataset, src_tokenizer):\n",
    "    for row in dataset:\n",
    "        yield src_tokenizer(row)\n",
    "\n",
    "def yield_tgt_tokens(dataset, tgt_tokenizer):\n",
    "    for row in dataset:\n",
    "        yield tgt_tokenizer(row)\n",
    "\n",
    "def buildVocab(yield_src_tokens, yield_tgt_tokens, src_tokenizer, tgt_tokenizer, normalized_data):\n",
    "    src_vocab = build_vocab_from_iterator(\n",
    "        yield_src_tokens(normalized_data['src'], src_tokenizer), \n",
    "        specials=['<unk>', '<pad>', '<sos>', '<eos>'], \n",
    "        max_tokens= 1000, \n",
    "        min_freq= 2)\n",
    "    src_vocab.set_default_index(src_vocab['<unk>'])\n",
    "\n",
    "    tgt_vocab = build_vocab_from_iterator(\n",
    "        yield_tgt_tokens(normalized_data['tgt'], tgt_tokenizer),\n",
    "        specials=['<unk>', '<pad>', '<sos>', '<eos>'],\n",
    "        max_tokens= 1000, \n",
    "        min_freq= 2)\n",
    "    tgt_vocab.set_default_index(tgt_vocab['<unk>'])\n",
    "\n",
    "    return src_vocab, tgt_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Số hóa các vector chữ cái đầu vào"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(normalized_data, src_tokenizer, tgt_tokenizer):\n",
    "    tokenized_data = {}\n",
    "    for column in normalized_data:\n",
    "        tokenized_data[column] = []\n",
    "        if column== 'src':\n",
    "            for row in normalized_data[column]:\n",
    "                tokenized_data[column].append(src_tokenizer(row))\n",
    "        if column== 'tgt':\n",
    "            for row in normalized_data[column]:\n",
    "                tokenized_data[column].append(tgt_tokenizer(row))\n",
    "\n",
    "    return tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTransform(vocab):\n",
    "    \"\"\"\n",
    "    Create the transform functions for the senquence.\n",
    "    \"\"\"\n",
    "    text_transform = T.Sequential(\n",
    "        T.VocabTransform(vocab),\n",
    "        T.AddToken(2, True),\n",
    "        T.AddToken(3, False)\n",
    "    )\n",
    "    return text_transform\n",
    "\n",
    "def transform_dataset(tokenized_data, src_vocab, tgt_vocab):\n",
    "    transformed_data = {}\n",
    "    \n",
    "    for column in tokenized_data:\n",
    "        if column== 'src':\n",
    "            transformed_data[column] = getTransform(vocab= src_vocab)(tokenized_data['src'])\n",
    "        if column== 'tgt':\n",
    "            transformed_data[column] = getTransform(vocab= tgt_vocab)(tokenized_data['tgt'])\n",
    "\n",
    "    return transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tokenizer = get_tokenizer(\"basic_english\")\n",
    "tgt_tokenizer = get_tokenizer(\"basic_english\")\n",
    "normalized_data = text_normalize(file_path)\n",
    "src_vocab, tgt_vocab = buildVocab(yield_src_tokens, yield_tgt_tokens, src_tokenizer, tgt_tokenizer, normalized_data)\n",
    "tokenized_data = tokenize_dataset(normalized_data, src_tokenizer, tgt_tokenizer)\n",
    "transformed_data = transform_dataset(tokenized_data, src_vocab, tgt_vocab)\n",
    "transformed_data = pd.DataFrame(transformed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Quá trình xây dựng DataLoader**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xây dựng Dataset và thực hiện load lên DataLoader phục vụ cho huấn luyện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, source_data, target_data):\n",
    "        \"\"\"\n",
    "        source_data: Danh sách các câu nguồn, mỗi câu là một danh sách các chỉ số (indices).\n",
    "        target_data: Danh sách các câu đích, mỗi câu là một danh sách các chỉ số (indices).\n",
    "        \"\"\"\n",
    "        self.source_data = source_data\n",
    "        self.target_data = target_data\n",
    "\n",
    "    def __len__(self):\n",
    "        # Trả về số lượng mẫu dữ liệu (số lượng cặp câu nguồn-đích)\n",
    "        return len(self.source_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Trả về cặp câu nguồn và đích tại vị trí idx\n",
    "        source_sentence = torch.tensor(self.source_data.iloc[idx], dtype=torch.long)\n",
    "        target_sentence = torch.tensor(self.target_data.iloc[idx], dtype=torch.long)\n",
    "        return source_sentence, target_sentence\n",
    "\n",
    "\n",
    "# Tạo DataLoader, sử dụng collate_fn để xử lý padding và drop cho các câu có độ dài không đồng đều\n",
    "def collate_batch(batch):\n",
    "    src_batch, tgt_batch = zip(*batch)\n",
    "    \n",
    "    # Padding cho các chuỗi nguồn và đích\n",
    "    src_batch = pad_sequence(src_batch, batch_first=True, padding_value= 1)\n",
    "    tgt_batch = pad_sequence(tgt_batch, batch_first=True, padding_value= 1)\n",
    "    \n",
    "    return src_batch, tgt_batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Đầu ra kết quả**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "#####################################################################\n",
    "# Phân chia các tập huấn luyện \n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.125\n",
    "isShuffle = True\n",
    "\n",
    "train_data, val_data = train_test_split(\n",
    "    transformed_data[['src', 'tgt']],\n",
    "    test_size= val_ratio,\n",
    "    random_state= 42,\n",
    "    shuffle= isShuffle\n",
    ")\n",
    "\n",
    "train_data, test_data = train_test_split(\n",
    "    train_data[['src', 'tgt']],\n",
    "    test_size= test_ratio,\n",
    "    random_state= 42,\n",
    "    shuffle= isShuffle\n",
    ")\n",
    "#####################################################################\n",
    "#####################################################################\n",
    "\n",
    "train_dataset = TranslationDataset(train_data['src'], train_data['tgt'])\n",
    "valid_dataset = TranslationDataset(val_data['src'], val_data['tgt'])\n",
    "test_dataset = TranslationDataset(test_data['src'], test_data['tgt'])\n",
    "\n",
    "# Khai báo số lượng câu trong 1 train, validation và test batch\n",
    "train_batch_size = 32\n",
    "test_batch_size = 8\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size= train_batch_size, collate_fn= collate_batch)\n",
    "val_loader = DataLoader(valid_dataset, batch_size= test_batch_size, collate_fn= collate_batch)\n",
    "test_loader = DataLoader(test_dataset, batch_size= test_batch_size, collate_fn= collate_batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
